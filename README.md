# Jusbrasil Crawler Challenge

> Resolu√ß√£o
> do [Desafio Software Engineer Intern](https://gist.github.com/diegoramosdev/4d6946efe20441d142e37f7510cbb3db)
---

## üíª Instalando depend√™ncias do projeto

A aplica√ß√£o foi desenvolvida em Python 3, utilizando a
biblioteca [BeautifulSoup4](https://beautiful-soup-4.readthedocs.io/en/latest/).

As depend√™ncias do projeto est√£o listadas no arquivo `requirements.txt`. Para instal√°-las utilizando o `pip`, basta
executar o comando a
seguir:

````commandline
pip install -r requirements.txt
````

---

## üöÄ Executando projeto localmente

> Por favor, nos informe quais s√£o as instru√ß√µes necess√°rias para rodar sua solu√ß√£o.

Para usar <nome_do_projeto>, siga estas etapas:

```
<exemplo_de_uso>
```

Adicione comandos de execu√ß√£o e exemplos que voc√™ acha que os usu√°rios achar√£o √∫teis. Fornece uma refer√™ncia de op√ß√µes
para pontos de b√¥nus!

---

## ‚ùó Informa√ß√µes complementares

> Nesta se√ß√£o est√£o respondidas demais informa√ß√µes solicitadas junto com a resolu√ß√£o do desafio.

### üìÜ Prazos:

- **Data de in√≠cio:** ``22/06/2023``
- **Data de conclus√£o:** ``26/06/2023``
- **Data de submiss√£o:** ``DD/06/2023``

### ‚è∞ Tempo investido:

- **Desenvolver os requisitos m√≠nimos da aplica√ß√£o:** ``7 horas e 20 min``

<img src="assets/forest.png " alt="Descri√ß√£o da imagem" width="350" height="350">

- **Documenta√ß√£o, refinamentos e melhorias:** ``X horas e Y min``

### üî® Ajustes e melhorias

> O que voc√™ n√£o incluiu em sua solu√ß√£o que gostaria que soub√©ssemos: Voc√™ estava com pouco tempo e n√£o conseguiu
> incluir algo?
> Outras informa√ß√µes sobre sua solu√ß√£o que voc√™ acha que seria importante sabermos (se aplic√°vel).

O projeto ainda est√° em desenvolvimento e as pr√≥ximas atualiza√ß√µes ser√£o voltadas nas seguintes tarefas:

- [x] Extrair dados
- [x] Salvar dados extra√≠dos num √∫nico arquivo JSON
    - [ ] Inserir caminho onde deve ser salvos os dados pelo terminal
- [x] Melhorar tratamento dos dados
- [ ] Enriquecer dados
    - [x] Valida√ß√£o do formato de datas
- [ ] Escrever documenta√ß√£o
    - [ ] README.md
    - [ ] Doc strings em fun√ß√µes
- [ ] Tratamento de exce√ß√µes
    - [ ] O que fazer se houver erro ao tentar escrever no arquivo JSON?
- [ ] Testes unit√°rios
- [ ] Aplica√ß√£o em paradgima POO

### üïµÔ∏è‚Äç‚ôÇÔ∏è Suposi√ß√µes assumidas:

> Use essa se√ß√£o para nos contar sobre qualquer suposi√ß√£o que tenha assumido ao criar sua solu√ß√£o.

Antes de construir o crawler, precisei pensar em estrat√©gias que poderia utilizar para extrair os dados solicitados da
p√°gina fornecida. Para isso, no primeiro momento, utilizei algum tempo para inspecionar a estrutura do HTML da p√°gina
com o intuito de perceber alguns padr√µes ou quais eram os n√≠veis de profundidade onde estavam essas informa√ß√µes.

Nesse processo, notei duas coisas me ajudaram a tra√ßar uma estrat√©gia:

1. As informa√ß√µes desejadas estavam dentro de tags `table` e estas, por sua vez, estavam agrupadas em uma `div`
   identificada por um ``id``. Assim, consegui mapear uma estrutura na qual poderia reduzir meu problema. De modo
   simplificado, segue a estrutura mapeada:

````html

<div id="divDadosResultado-A">
    <table>
        <tr>
            <td></td> <!-- Index da tabela que continha as informa√ß√µes = Informa√ß√£o irrelevante para o caso -->
            <td>
                <span></span>
                <table> <!-- Tabela com as informa√ß√µes de jurisprud√™ncia -->
                    <tr>
                        <td>
                            <!-- Outras tags -->
                        </td>
                    </tr>
                    <tr>
                        <td></td>
                    </tr>
                    <!-- Outras tags tr -->
                </table>
            </td>
        </tr>
    </table>
</div>
````

2. Analisando a primeira tag ``<table>`` desta ``div``, √© poss√≠vel perceber que suas linhas s√£o compostas de n√∫mero que
   representa o √≠ndice, e de outras tabelas, onde est√£o localizadas as informa√ß√µes das jurisprud√™ncias. Nessas
   tags `table`, pude notar que todas as informa√ß√µes que precisaria extrarir estavam dentro de uma "etiquetas"
   (**Ementa:**, **Classe / Assunto:**, etc.). Logo, poderia utilizar esse padr√£o para extrair a informa√ß√£o. Nesse ponto
   havia outros dois poss√≠veis caminhos:
    1. Navegar pelo HTML at√© o n√≠vel da ``<div id="divDadosResultado-A">`` e, ent√£o, utilizar Express√µes Regulares a fim
       de identificar o padr√£o dessas etiquetas ou
    2.

Se n√£o fossem essas "etiquetas", seria necess√°rio identificar algum outro padr√£o a n√≠vel de tag para que, usando o
BeautilfulSoup, pudesse atingir as informa√ß√µes.

No arquivo JSON, cada objeto que representa um jurisprud√™ncia possui os atributos ``dataPublicacao``
e ``dataJulgamento``. Como essas informa√ß√µes s√£o obtidas como strings durante a extra√ß√£o, julguei interessante criar uma
fun√ß√£o respons√°vel por verific√°-las a fim de saber se tratam de datas v√°lidas ou n√£o, armazenando esta
informa√ß√£o em atributos extras, nomeados de ``_dataPublicacaoValida`` e ``_dataJulgamentoValida``, respectivamente.

### üîÄ Atalhos realizados:

> Se aplic√°vel, voc√™ fez algo que sentiu que poderia ter feito melhor em uma aplica√ß√£o do "mundo real"?

Como neste cen√°rio existia esse padr√£o das "etiquetas", preferi adotar essa estrat√©gia. Por√©m, algumas vezes me
questionei se num cen√°rio do "mundo real" o ideal seria n√£o depender desses padr√µes textuais e fosse prefer√≠vel fazer um
crawler que navegasse a n√≠vel de estrutura de p√°gina.

### üí¨ Feedbacks:

> Voc√™ tem algum feedback para tornarmos esse desafio melhor? Por favor, nos conte. :)

At√© que me diverti fazendo o desafio. Achei tanto o objetivos quanto as orienta√ß√µes bem claras. Ent√£o, semana de
mesmo estando em semana de avalia√ß√µes na faculdade (final de semestre) isso me ajudou aentender o que deveria ser feito.

Al√©m disso, aproveitei a oportunidade para iniciar a leitura do
livro [Web Scraping com Python](https://www.amazon.com.br/Web-Scraping-Com-Python-Coletando/dp/8575227300/ref=sr_1_8?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=3LL9RB4MO701K&keywords=python&qid=1687927515&sprefix=pytho%2Caps%2C232&sr=8-8),
que havia comprado h√° um tempinho atr√°s. Ent√£o, fui aprendendo algumas coisas no caminho (Primeira vez que apliquei
regex) e foi interessante ver que consegui p√¥r ... num desafio pr√°tico.

[‚¨Ü Voltar ao topo](#jusbrasil-crawler-challenge)<br>

