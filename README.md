# Jusbrasil Crawler Challenge

> Resolu√ß√£o
> do [Desafio Software Engineer Intern](https://gist.github.com/diegoramosdev/4d6946efe20441d142e37f7510cbb3db)
---

## üöÄ Executando o projeto localmente

> Por favor, nos informe quais s√£o as instru√ß√µes necess√°rias para rodar sua solu√ß√£o.

### üíª Instalando depend√™ncias do projeto

A aplica√ß√£o foi desenvolvida em **Python 3**, utilizando a vers√£o ``3.13.3`` da linguagem. A biblioteca
[**BeautifulSoup4**](https://beautiful-soup-4.readthedocs.io/en/latest/), na vers√£o ``4.12.2``, tamb√©m foi utilizada.

As depend√™ncias do projeto est√£o listadas no arquivo `requirements.txt`.

### ü§ñ Executando o crawler

Para executar o projeto, siga estas etapas:

1. Ap√≥s baixar o projeto, abra um terminal dentro da pasta raiz (``crawler-jusbrasil-challenge``), execute o comando
   abaixo para instalar as depend√™ncias utilizando
   o `pip`:

````shell
pip install -r requirements.txt
````

2. Depois que as depend√™ncias forem instaladas, ainda no terminal, execute o comando a seguir:

```
python main.py
```

3. Pronto! Seus dados estar√£o salvos no arquivo `result.json`, dentro do diret√≥rio ``docs`` (n√£o se preocupe: caso o
   diret√≥rio ou o arquivo ainda n√£o existam, eles ser√£o criados na primeira vez que executar o programa). Abaixo est√° um
   exemplo do JSON salvo:

```json
[
  {
    "_dataExtracao": "2023-06-29 13:02:30.988729",
    "_dataJulgamentoValida": true,
    "_dataPublicacaoValida": true,
    "_id": 1,
    "assunto": "Apela√ß√£o C√≠vel / Banc√°rios",
    "comarca": "Santa F√© do Sul",
    "dataJulgamento": "26/01/2022",
    "dataPublicacao": "26/01/2022",
    "ementa": "RESPONSABILIDADE CIVIL ‚Äì A√ß√£o declarat√≥ria de inexigibilidade de d√©bito c.c. indeniza√ß√£o por dano moral ‚Äì Contrata√ß√£o n√£o reconhecida pelo autor ‚Äì Origem do d√©bito n√£o comprovada ‚Äì Falha na presta√ß√£o do servi√ßo ‚Äì Responsabilidade objetiva da prestadora de servi√ßos ‚Äì Risco profissional ‚Äì Dano moral bem caracterizado ‚Äì Indeniza√ß√£o arbitrada segundo o crit√©rio da prud√™ncia e razoabilidade",
    "numeroProcesso": "1001143-04.2021.8.26.0541",
    "orgaoJulgador": "20¬™ C√¢mara de Direito Privado",
    "relator": "Correia Lima"
  },
  {
    ...
  }
]
```

---

## ‚ùó Informa√ß√µes complementares

> Nesta se√ß√£o est√£o respondidas demais informa√ß√µes solicitadas junto com a resolu√ß√£o do desafio.

### üìÜ Prazos:

- **Data de in√≠cio:** ``22/06/2023``
- **Data de conclus√£o:** ``26/06/2023``
- **Data de submiss√£o:** ``29/06/2023``

### ‚è∞ Tempo investido:

<img src="assets/forest_tempdev.png " alt="Gr√°fico com tempo utilizado para desenvolvimento" width="350" height="350">

- **Tempo para desenvolver os requisitos m√≠nimos da aplica√ß√£o:** ``7 horas e 20 min``

<img src="assets/forest_temptotal.jpg " alt="Gr√°fico com tempo utilizado para desenvolvimento" width="350" height="350">

- **Tempo total (incluindo documenta√ß√£o, refinamentos e melhorias):** ``10 horas e 24 min``

---

### üî® Requisitos b√°sicos, ajustes e melhorias

> O que voc√™ n√£o incluiu em sua solu√ß√£o que gostaria que soub√©ssemos: Voc√™ estava com pouco tempo e n√£o conseguiu
> incluir algo?
> Outras informa√ß√µes sobre sua solu√ß√£o que voc√™ acha que seria importante sabermos (se aplic√°vel).

**Requisitos b√°sicos:**

- [x] Extrair dados
- [x] Salvar dados extra√≠dos em um √∫nico arquivo JSON
- [x] Escrever ``README.md``

**Ajustes e melhorias:**

- [x] Melhorar tratamento dos dados
- [x] Enriquecer dados
    - [x] Valida√ß√£o do formato de datas
    - [x] Adicionar informa√ß√£o de quando a extra√ß√£o foi feita
- [x] Tratamento de exce√ß√µes
    - [ ] O que fazer se houver erro ao tentar escrever no arquivo JSON?
- [ ] Testes unit√°rios
- [ ] Aplica√ß√£o em paradgima POO

### üïµÔ∏è‚Äç‚ôÇÔ∏è Suposi√ß√µes assumidas:

> Use essa se√ß√£o para nos contar sobre qualquer suposi√ß√£o que tenha assumido ao criar sua solu√ß√£o.

Antes de construir o crawler, precisei pensar em estrat√©gias que poderia utilizar para extrair os dados solicitados da
p√°gina fornecida. Para isso, no primeiro momento, utilizei algum tempo para inspecionar a estrutura do HTML da p√°gina
com o intuito de perceber alguns padr√µes ou quais eram os n√≠veis de profundidade onde estavam essas informa√ß√µes.

Nesse processo, notei duas coisas me ajudaram a tra√ßar uma estrat√©gia:

1. As informa√ß√µes desejadas estavam dentro de tags `table` e estas, por sua vez, estavam agrupadas em uma `div`
   identificada por um ``id``. Assim, consegui mapear uma estrutura na qual poderia reduzir meu problema. De modo
   simplificado, segue a estrutura mapeada:

````html

<div id="divDadosResultado-A">
    <table>
        <tr>
            <td></td> <!-- Index da tabela que continha as informa√ß√µes = Informa√ß√£o irrelevante para o caso -->
            <td>
                <span></span>
                <table> <!-- Tabela com as informa√ß√µes de jurisprud√™ncia -->
                    <tr>
                        <td>
                            <!-- Outras tags -->
                        </td>
                    </tr>
                    <tr>
                        <td></td>
                    </tr>
                    <!-- Outras tags tr -->
                </table>
            </td>
        </tr>
    </table>
</div>
````

2. Analisando a primeira tag ``<table>`` desta ``div``, √© poss√≠vel perceber que suas linhas s√£o compostas de n√∫mero que
   representa um √≠ndice, e de outras tabelas, onde est√£o localizadas as informa√ß√µes das jurisprud√™ncias. Nessas
   tags `table` mais internas, pude notar que todas as informa√ß√µes que precisaria extrarir estavam dentro de uma "
   etiquetas" (**Ementa:**, **Classe / Assunto:**, etc.). Logo, poderia utilizar esse padr√£o para extrair a
   informa√ß√£o. Nesse ponto havia outros dois poss√≠veis caminhos:
    1. Navegar pelo HTML at√© o n√≠vel da ``<div id="divDadosResultado-A">``, localizar a tabela agregadora das demais
       tags `<table>`, percorrer as tags `tr` dessas tabelas internas e ent√£o utilizar Express√µes Regulares a fim
       de identificar o padr√£o dessas "etiquetas" e considerar os textos ap√≥s essas; ou
    2. Se n√£o fossem essas "etiquetas", seria necess√°rio identificar algum outro padr√£o a n√≠vel de _tag_ para que,
       usando o BeautilfulSoup, pudesse atingir as informa√ß√µes.

Optei pela op√ß√£o ii, uma vez as _tags_ mais internas das tabelas que continham os dados de jurisprud√™ncia n√£o seguiam
um padr√£o muito bem definido, tornando a op√ß√£o ii um pouco mais invi√°vel para o objetivo.

As informa√ß√µes de data me pareceram relevantes. Portanto, julguei que seria interessante realizar uma valida√ß√£o sobre
elas. Para isso, no arquivo JSON, cada objeto que representa uma jurisprud√™ncia possui os atributos ``dataPublicacao``
e ``dataJulgamento``. Como essas informa√ß√µes s√£o obtidas como strings durante a extra√ß√£o, criei uma fun√ß√£o respons√°vel
por verific√°-las a fim de saber se tratam de datas v√°lidas ou n√£o, armazenando esta informa√ß√£o em atributos extras,
nomeados de ``_dataPublicacaoValida`` e ``_dataJulgamentoValida``, respectivamente.

Essa abordagem me levou a criar um outro atributo para ser armazenado junto com a juriprud√™ncia extra√≠da: a data em que
a extra√ß√£o foi feita. Por isso, tamb√©m acrescentei o atributo ``_dataExtracao`` aos objetos antes de serem salvos no
arquivo JSON.

### üîÄ Atalhos realizados:

> Se aplic√°vel, voc√™ fez algo que sentiu que poderia ter feito melhor em uma aplica√ß√£o do "mundo real"?

Como neste cen√°rio existia esse padr√£o das "etiquetas", preferi adotar essa estrat√©gia, conforme explicado acima. Por√©m,
algumas vezes me questionei se num cen√°rio do "mundo real" o ideal seria n√£o depender desses padr√µes textuais e fosse
prefer√≠vel fazer um crawler que navegasse a n√≠vel de estrutura de p√°gina.

Al√©m disso, estamos considerando um cen√°rio no qual o crawler "varre" uma √∫nica p√°gina. Em uma aplica√ß√£o do "mundo real"
seria importante que o crawler fosse capaz de navegar entre outras p√°ginas, procurando outras informa√ß√µes de
jurisprud√™ncia. Por√©m, n√£o _ao infinito e al√©m_, mas por tempo o suficiente para que pudesse percorrer as p√°ginas que
continham essas informa√ß√µes.

Para este cen√°rio tamb√©m n√£o me preocupei com a sobrecarga que o crawler poderia gerar. Mas, acredito que numa
aplica√ß√£o "real" isso deve ser um ponto relevante.

Por fim, o crawler desenvolvido n√£o √© tolerante √† falhas; apesar de haver tratamento de exe√ß√£o ou outro.

### üí¨ Feedbacks:

> Voc√™ tem algum feedback para tornarmos esse desafio melhor? Por favor, nos conte. :)

At√© que me diverti fazendo o desafio. Achei tanto o objetivos quanto as orienta√ß√µes bem claras. Ent√£o, mesmo estando em
semana de avalia√ß√µes de final de semestre na faculdade, isso me ajudou entender o que deveria ser feito.

Al√©m disso, aproveitei a oportunidade para iniciar a leitura do
livro [Web Scraping com Python](https://www.amazon.com.br/Web-Scraping-Com-Python-Coletando/dp/8575227300/ref=sr_1_8?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=3LL9RB4MO701K&keywords=python&qid=1687927515&sprefix=pytho%2Caps%2C232&sr=8-8),
que havia comprado h√° um tempinho atr√°s. Fui aprendendo algumas coisas no caminho e foi interessante ver que consegui
aplicar num desafio pr√°tico. Esse desafio tamb√©m acabou me permitindo experimentar algumas coisas pela primeira vez,
como foi o caso das express√µes regulares.

[‚¨Ü Voltar ao topo](#jusbrasil-crawler-challenge)<br>

